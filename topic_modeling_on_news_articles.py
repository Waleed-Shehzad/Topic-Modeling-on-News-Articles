# -*- coding: utf-8 -*-
"""Topic Modeling on News Articles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RqVE25kKYxx9-hyGonhPQ1oMNO246Rk0

# Install & Import Libraries
"""

!pip install kagglehub gensim pyLDAvis wordcloud nltk spacy scikit-learn

import kagglehub
import pandas as pd
import numpy as np
import nltk
import re
import gensim
import gensim.corpora as corpora
from gensim.models import LdaModel, CoherenceModel
from sklearn.decomposition import NMF
from sklearn.feature_extraction.text import TfidfVectorizer
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import spacy

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

!python -m spacy download en_core_web_sm
nlp = spacy.load("en_core_web_sm")

"""# Load Dataset"""

path = kagglehub.dataset_download("gpreda/bbc-news")
print("Path:", path)

df = pd.read_csv(path + "/bbc_news.csv")
df.head()

"""# Text Preprocessing"""

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    tokens = nltk.word_tokenize(text)
    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]
    doc = nlp(" ".join(tokens))
    tokens = [token.lemma_ for token in doc if token.lemma_ not in stop_words]
    return tokens

df['tokens'] = df['description'].astype(str).apply(preprocess_text)
df.head()

"""# Prepare Data for LDA"""

id2word = corpora.Dictionary(df['tokens'])
corpus = [id2word.doc2bow(text) for text in df['tokens']]

print("Sample BOW:", corpus[0][:10])

"""# Train LDA Model"""

num_topics = 5

lda_model = LdaModel(corpus=corpus,
                     id2word=id2word,
                     num_topics=num_topics,
                     random_state=42,
                     update_every=1,
                     chunksize=100,
                     passes=10,
                     alpha='auto',
                     per_word_topics=True)

for idx, topic in lda_model.print_topics(-1):
    print(f"Topic {idx}:\n {topic}\n")

"""# Visualize LDA Topics (pyLDAvis)"""

pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, corpus, id2word)
vis

"""# Compare LDA vs NMF"""

tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(df['description'].astype(str))

nmf_model = NMF(n_components=num_topics, random_state=42)
nmf_model.fit(tfidf)

feature_names = tfidf_vectorizer.get_feature_names_out()

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))
        print()

display_topics(nmf_model, feature_names, 10)


print(" LDA captures probabilistic word-topic distributions, good for interpretability.")
print(" NMF is faster and works well with TF-IDF, often producing sharper topics.")